\documentclass[12pt,twoside]{reedthesis}
\usepackage[hyphens]{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{latexsym} 
\usepackage{longtable}
\usepackage{rotating}
\usepackage{setspace} 
\usepackage{xcolor} 
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{ulem} % strikethrough
%\usepackage{natbib}
%\usepackage[style=reading,backend=bibtex8]{biblatex} \addbibresource{bibliography.bib}
\setlength{\parskip}{0pt}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\green}[1]{\textcolor{olive}{#1}}
\newcommand{\todo}{\red{TODO}}
\newcommand{\comment}[2]{\textbf{#1} \textcolor{blue}{#2}}
\newcommand{\addressed}[2]{{#1}}

\title{An Overview of Superoptimizers}
\author{Elijah Wheelock}
\date{May 2024}
\division{Mathematics and Natural Sciences}
\advisor{Greg Anderson}
\department{Computer Science}
\begin{document}
\maketitle
\frontmatter % this stuff will be roman-numbered
\pagestyle{empty} % this removes page numbers from the frontmatter
\chapter*{Acknowledgements} % Acknowledgements are optional
I'd like to thank Greg, Tobey, Jennifer, Arthur, Norman, Susan, and Elise.
% \chapter*{Preface} % The preface is optional
\tableofcontents

\chapter*{Abstract} % If your abstract is longer than a page, there may be a formatting issue.
    Superoptimization is a family of compilation techniques which seek to improve the characteristics of computer programs by computational means.
    This is seen as distinct from "ordinary" optimization (perhaps more properly called meliorization), which seeks to improve programs by application of a large number of rewrite rules.
    Another view is that superoptimization recharacterizes the problem of program improvement as a search problem, rather than an equational reasoning problem.
    This is desirable because it allows the use of a wide variety of techniques from the literature on unguided search problems.
    More broadly, superoptimization may be seen as an application of the techniques of program synthesis to the domain of concretely-realized (assembly code/IR) programs.
    In practice, there is an infinite number of potential programs which compute the desired result, many of which are not intuitively obvious.
    Therefore it is desirable to have special techniques for synthesizing (generating), pruning, and verifying such candidate programs. 
    In this paper, we will survey a variety of methods from the literature that have been used for these ends.

\mainmatter % here the regular arabic numbering starts
\pagestyle{fancyplain} % turns page numbering back on
\chapter*{Introduction} % The \introduction command is provided as a convenience. % if you want special chapter formatting, you'll probably want to avoid using it altogether
    \addcontentsline{toc}{chapter}{Introduction} % intro gets included in the table of contents
    \chaptermark{Introduction}
    \markboth{Introduction}{Introduction} % make sure that the headers are right. % don't number intro as 1 so that chapter one is 1.
    \singlespacing % \onehalfspacing % \doublespacing
    
    \red{TODO:} cite some stuff! E.g. Turing on undecidability. probably anything a CS senior wouldn't know should be cited
    \\ \red{TODO:} Reiterate abstract in introduction
    \\ \red{TODO:} add a bit more introduction about what program optimization is and how and why it is done classically
    \\ \red{TODO:} lay out a complete/formal problem statement. 
    
    All these criteria represent possible desiderata in modifying our programs.
    That is to say, these criteria are also the motivations behind writing optimizers and superoptimizers. 
    
    In writing any kind of optimizer, one must first select a criterion by which to rank candidate programs.
    There are
        \comment{a few potential measures by which to do so}{Related to the comment above, these are independent of superoptimization so I would introduce them first in the context of classical optimization}
        .
    Perhaps the most immediately salient is performance: 
        in almost every application, faster programs are preferred to slower ones.
    Another common goal is to reduce resource use: 
        for example, a program which needs less memory is
            \comment{preferable}{typical practice is to find something to cite for these kinds of claims (maybe just pick some paper that introduce a technique for minimizing memory usage)}
            to one which uses more.
    There are also other resources whose conservation might be desired, for example 
    \begin{itemize}
        \item functional units on an FPGA, \comment{}{Similarly, citations here would be ideal (but I wouldn't make this a high priority)}
        \item power or space on a hardware design, 
        \item disk writes, 
        \item or disk or network bandwidth.
    \end{itemize}
    A less-common goal is to reduce the code size of generated programs.
    This might be desired for reasons such as:
    \begin{itemize}
            \item performance (reducing instruction cache misses or instruction pipeline delays), 
            \item the output code is meant to run on a resource-poor system such as a microcontroller, 
            \item entertainment ("code golfing").
    \end{itemize}
    
    \red{TODO:} introduce the general framework of synthesis/verification/pruning explicitly
    
    Since the introduction of the term by \cite{massalin1987superoptimizer}, a wide variety of techniques have been harnessed to the goal of superoptimization.
    These techniques may be broadly categorized in several ways, including:
        \begin{itemize}
            \item the goals (optimization criteria) of the superoptimization system,
            \item the techniques used for synthesis and/or verification of generated programs,
            \item the problem domain,
            \item the capabilities/limitations of the system.
        \end{itemize}
    In this paper we will focus on categorizing systems by synthesis technique, but first it seems advisable to briefly cover the other possible taxonomies.
    
    % Motivation: We want to improve these criteria! Superoptimization is one way to do that!
    Another possible taxonomy is by problem domain.
    Many superoptimizers focus on assembly programs with integer operands, because there are powerful techniques for verifying such programs, but by no means all.
    There have also been interesting superoptimizers targeting 
        floating-point assembly programs, 
        compiler intermediate representation (IR) programs, 
        and high-level synthesis (HLS) languages for integrated circuit (IC) or field-programmable gate array \comment{(FPGA)}{readers won't remember all of these abbreviations in the later chapters, so I would wait to introduce them until they become relevant later} development.
    
    A related way to categorize superoptimizers is by the limits they set on their problem domain.
    For example, a common limitation is that the output code must be expressible in a loop-free way
        -- loops are a difficult problem for many approaches,
            not least because verifying equivalence of programs with loops is in general undecidable,
            and because heuristic testing of loops may require unbounded time.
    Another common problem in superoptimizers is that it's very difficult to produce an accurate cost estimation for memory access.
        Modern computer systems generally have many-layered information storage systems, with multiple layers of caching, batching, and indirection between them, making the estimation of latency for any particular memory access very difficult.
        For this reason, \comment{black-box measurement}{by this do you mean measuring time by running the actual program on actual hardware? If so I would say that since black-box can mean a lot of different things.} of performance is the gold standard, but this is sometimes cost-prohibitive.
    Finally, the overall difficulty of any search problem scales with the size of the search space, which in superoptimization scales exponentially with the length of the synthesized program.
        Therefore most superoptimization systems are limited mainly by available computation power, and impose a maximum synthesized program size for performance reasons.
    
    \red{TODO} talk about difficulties synthesizing constants, memory access, %TODO 
    \red{TODO} talk about guaranteed progress, timeouts, etc.
    
    There are also multiple approaches for verification of synthesized programs.
    Perhaps the simplest is heuristic testing: 
        directly measuring the output of the synthesized program against a set of test cases, usually generated by the input (unoptimized) program.
    Another approach is boolean verification: using a SAT solver to prove bitwise equivalence of the program's inputs and outputs.
    This is potentially very expensive, but if done correctly guarantees the correctness of the generated program. 
    
    Most interestingly for our purposes, superoptimizers can be categorized by the approach they take to synthesis of superoptimized programs.
    This is the measure on which published systems vary most widely, and the selection of approach is perhaps the most salient question for the authors of new systems.
    These approaches vary widely in sophistication, complexity, and power.
    
    Perhaps the simplest possible approach, pioneered by \cite{massalin1987superoptimizer}, is simple enumeration. 
    The technique is to line up all possible programs in order of length, testing each in turn by the selected verification strategy. 

\chapter{Symbolic Search}
    \red{TODO:} Chapter intro: what is symbolic search?
    \subsection{Enumeration}
        \subsubsection{Superoptimizer -- A Look at the Smallest Program}
            Happily, the earliest published superoptimizer is also among the simplest, and so will serve admirably as an introduction to the idea.
            The word "superoptimizer" dates from Henry Massalin's paper, "Superoptimizer -- A Look at the Smallest Program" \cite{massalin1987superoptimizer}.
            Massalin's superoptimizer, like most superoptimizers, operates in three main phases: synthesis, pruning (optional), and verification.
                
            The synthesis stage in this system is, perhaps, the simplest imaginable: simply write down all assembly programs in alphabetical order.
            The advantage of this approach is that if a result is found, it will be the smallest possible code that can compute the desired result.
            In the loop-free case, this is likely also the fastest result -- running fewer instructions is generally preferable.
            However, on many architectures, the latency for instructions varies -- for example, division may be more expensive than multiplication, which may be more expensive than addition \cite{fog1996instructiontables}.
            The disadvantage is that the search space blows up exponentially with the length of function sought, making it intractable for all but the smallest code segments.
                
            The pruning stage is also relatively simple.
            There are some obviously-spurious code sequences if what is sought is the shortest possible correct code, e.g. "move X,Y; move Y,X".
            Code containing such sequences may be rejected early, before the more-expensive verification stage.
                
            Massalin gives two distinct approaches to verification, driven by resource limitations.
            The reliable, but slow and expensive way, is to logically test the bitwise equivalency of the candidate program to the desired program, using what Massalin calls a "boolean program verifier".
            As we'll see soon, this approach has become much more practical in the years since then, due to improvements both in hardware and SMT solvers. 
                
            \green{
            This system, although relatively simple, and harshly limited by the available computation hardware of its era, achieves some qualitatively interesting results -- for example, Massalin uses it to search for efficient instruction sequences to divide by constants, but fails to find any.
            Since the superoptimizer performs an exhaustive search, this constitutes a proof that there are no such sequences. (Presumably these constants were not powers of two, division by which can be accomplished by a right shift.)
            }

        \subsubsection{Automatic Generation of Peephole Superoptimizers}
            One clever way to reduce the expense and inconvenience associated with running a superoptimizer to improve your code is to precompute it
            -- that is, to cache the results of superoptimizing common code segments, and use these results as part of the peephole stage of an ordinary optimizer.
            Such is the strategy adopted by Bansal and Aiken \cite{bansal2006peephole}.
            
            This system works by harvesting code segments from a large corpus of existing programs, with certain restrictions, and superoptimizing them. 
            In contrast to Massalin's superoptimizer, it uses a runtime-estimate based cost function, as opposed to length-based.
            Rather than simply counting up the number of instructions, it adds up their cycle times, as referenced from the Intel technical manual.
            This gives a better approximation of the true runtime of the code segment.
            
            The selection of code segments is somewhat involved.
            One problem that might occur with superoptimizing a code segment is that it might disturb invariants relied on by other parts of the program.
            When the code segment is selected manually, this is not a problem.
            But for automatic selection, the system has to be confident that the segments won't be jumped into from elsewhere.
            For example, if the superoptimizer were to work on a code segment containing the start of a loop, but not the end, it would become difficult where the target of the jump at the end of the loop should be, especially if instructions are deleted or reordered.
            Another problem that could occur is that the update of the loop index variable could be lost or moved, in which case the semantics of the overall program are most likely made nonsense.
            
            It also uses a different pruning strategy.
            Before testing an instruction sequence, it is first canonicalized by renaming the registers it uses in a consistent way.
            If the canonicalized form has already been checked, then the sequence may be safely pruned.
            
            The verification strategy is similar to Massalin's boolean verifier, but using a SAT solver.
            This approach had become much more practicable in the intervening time between these publications due to algorithmic and hardware advances -- see e.g. \cite{silva1996grasp}.
            
            \green{ % this is not actually newly-written, but I'm highlighting it to go along with the other new results sections
            Bansal and Aiken's system achieves quite impressive results -- a speedup by a factor of between 1.7 and 10, (mean ~4.9) in compute-intensive programs, with a 1-5\% speedup in more general programs, all compared to code already nominally optimized by the compiler.
            }

    \subsection{SMT}
        \red{TODO}: add background, SAT solver
    \\  \red{TODO}: graph rewriting
        
        \subsubsection{Denali: A Goal-directed Superoptimizer}
            \red{TODO:} mapping of E-graph to SAT clause
            Another early work on superoptimization, by Joshi et al. \cite{joshi2002denali} uses an automated theorem prover to simultaneously synthesize and verify new programs.
            It takes a program in a custom low-level language, expresses it in a symbolic form as a graph of all equivalent ways of computing its output expressions
                \addressed{called the E-graph}{I would be inclined to have a separate description of E-graphs, but it depends on the structure of the section},
                % I don't think any of the other papers I'm covering use E-graphs (directly),
                % so I think it makes sense to define the term closest to its use.
                %   (I believe this type of E-graph reasoning got merged into SMT solvers,
                %    so later papers which use SMT solvers get the benefits for free.)
                and formulates the conjecture that ``no program of the target architecture computes the values of the goal terms within K cycles"
                    \comment{as an input to a satisfiability solver}{I would reorganize slightly so that this comes right before the explanation below}. 
                
            The \comment{graph re-writing}{introduce graph rewriting} uses a fairly standard term-rewriting strategy\footnotemark with the addition of a notion of equivalence for graph nodes.
            The connections between the E-graph nodes are then translated into clauses for the use of the SAT solver.
                
            \footnotetext{ -- that is to say, like a compiler, it has a set of rules by which it re-writes expressions. For each rule, it searches the document for that rule's the left-hand-side, and replaces it with the right-hand-side, and repeats until no more substitutions are possible.}
                
            The satisfiability step requires a little more detail.
            In order to logically express the number of cycles required for the whole program, it has to express the time by which every intermediate step will be completed.
            To do this, Denali uses an additional set of constraints on the times of launching, completion, and availability, which may be roughly summarized by the following slogans:
            \begin{itemize}
                \item A computation is completed after its latency has elapsed since its launch time.
                \item A computation can't be launched until its arguments are available.
                \item A value is available at all times at or after its completion time.
                \item Only one operation may be launched per cycle (The paper gives some remarks on extending to multiprocessing systems, but does not explain in detail).
            \end{itemize}
                
            \green{
            The results section in this paper is not extensive, but they report that Denali was able to meet or exceed the performance of an aggressively-optimizing C compiler on certain problems.
            }

        \subsubsection{Unbounded Superoptimization}
            A more modern take on a similar approach to Denali is the work by Jangda and Yorsh in "Unbounded Superoptimization" \cite{jangda2017unbounded}.
            Rather than matching E-graphs, which in the intervening years had been integrated into SMT solvers, it to directly encodes the target program as a logical formula.
            It does this by encoding the input program, treating it as the first candidate output, and
                \comment{"squeezing" it by imposing lower and lower cost bounds on the solver}{I would make this a bit more explicit--how exactly are computational bounds encoded and how does the tool decide what bounds to use?}.
            This "top-down" approach has the advantage that if the superoptimizer is halted early, it can still return a correct output, unlike many superoptimizers which work "bottom-up" and thus start without any candidate programs.

            \green{
            Jangda and Yorsh tested their technique on a set of problems from Hacker's Delight \cite{warren2013hackers}.
            They successfully generated optimal code for 17 of 25 problems, and failed for 8.
            Comparing the output of their technique to the outputs of \texttt{gcc -O3}, they found that for 3 of the problems the code generated by their tool was about 20\% faster than (and algorithmically distinct from) the output of \texttt{gcc -O3}; on the other 14 problems the performance was identical to that of \texttt{gcc -O3}.
            }

    \subsection{CEGIS}
        Counter-example guided inductive synthesis (CEGIS) is a program synthesis technique introduced by Solar-Lezama et al. in \cite{solar-lezama2006sketch}.
        Like some of the other techniques we've seen so far, it
        \\  \red{TODO}: explain CEGIS (briefly-ish) (each counterexample rules out at least one program, so you mostly always make progress, usually quickly).

        \subsubsection{Souper: A Synthesizing Superoptimizer}
            Souper \cite{sasnauskas2017souper} is an engineering project to create a superoptimizer working on LLVM IR.
            This is desirable because LLVM is used as the "middle end" for a large number of compiler projects, and therefore Souper can be used for a wide variety of languages and target architectures.
            It has three main components:
                an extractor, to produce Souper IR from LLVM IR,
                a synthesizer using counterexample-guided interactive synthesis,
                and a verifier using an SMT solver.
            
            \red{TODO}: cite Gulwani et al. "Synthesis of loop-free programs"?
            
            Souper uses a custom IR analogous to a purely-functional subset of LLVM IR without control flow\footnotemark, represented for computational purposes as a directed acyclic dataflow graph.
            It is also limited to integer instructions; it has no model for floating-point, memory, or vector operations.
            The operands of programs in this IR are considered only as bitvectors (parameterized by width) or tuples of bitvectors.
            
            \footnotetext{Some information about control flow external to the code segment under consideration is preserved, but control flow operations within a code segment are forbidden.}
            
            \red{TODO:} a bit on the synthesizer
            
            \red{TODO}: Threats to soundness:
            \\ \comment{Souper bugs}{Bugs in the synthesis tool itself are common to all approaches--you can bring it up here since Souper is purportedly more ``production-ready'' than others but I would make it clear that this is not unique to Souper},
            \\ LLVM bugs,
            \\ \comment{Solver bugs}{Same as the comment above. I would consider separating these two from the other two, since LLVM bugs and undefined behavior are more unique to Souper},
            \\ Undefined behavior: One issue with relying on the semantics of LLVM is that there are \comment{areas of undefined behavior}{elaborate}!

        \subsubsection{Dataflow-Based Pruning for Speeding up Superoptimization}
            Mukherjee et al. \cite{mukherjee2020dataflow} enhance Souper with an additional family of pruning strategies based on dataflow reasoning. 
            Pruning spurious candidates as early as possible is obviously highly desirable, because every hole for a constant represents a potentially-costly invocation of the MST solver; a hole for a partial program is even worse, as it represents a possibly-infinite family of related candidates.
            This paper uses both \textit{forward} and \textit{backward} dataflow analysis techniques to prove candidate expressions incorrect before they are fully substantiated (while they still contain "holes", constants or sub-expressions which have yet to be synthesized).
            Mukherjee et al. define forward techniques as those which work by drawing a chain of reasoning from properties of the specification program's output into properties which must hold for a correct candidate program.
            By contrast, backward techniques draw inferences from the program's inputs into the candidate.
            
            The forward techniques are known/bivalent bits and integer ranges.
            The backward techniques are required bits/don't-care bits and forced bits.
            
            The known-bits analysis works by proving that certain bits of the output of the candidate program are necessarily incompatible with the specification.
                For a simple example, if the specification ends by setting the rightmost bit of the output to 1, then any candidate which ends with a left shift can be immediately dismissed.
            The integer ranges analysis works by proving that there is some output of the specification that is outside the range of the candidate.
                For example, if we know that for the specification $f(x) = -1$ for some $x$, we can prune the candidate $g(x) = abs(x)$.
            
            The required-bits analysis works by proving that certain bits of the input to the specification are \textit{required}, i.e. that changing its value (with other bits constant) necessarily changes the output.
                If such an input is not used in a candidate program (is a don't-care bit), then that program can be safely pruned.
                    For example, if the specification is $x + y$, but the candidate is $x + C$ where $C$ is constant, the candidate may be pruned.
            The forced-bits analysis is in a way analogous to the known-bits analysis, working in the opposite direction.
                It works by finding conflicts in candidates with symbolic (not-yet-synthesized) constants.
                Intuitively, it can be thought of as finding inconsistencies in a system of equations based on comparing the candidate to the specification.
                For example, suppose the specification is $f(x) = x^2 + 1$, and the candidate is $g(x) = x + C$. In that case, $f(3) = 10$, and $f(1) = 2$. Then we need $C$ such that $f(3) = g(3) = 10 = 3 + C$, and $f(1) = g(1) = 2 = 1 + C$, so we must have $1 = C = 7$. This is unsatisfiable, so we can dismiss $g$ without attempting to learn anything more about it.
            
            \green{
            Results: Mukherjee et al. tested the performance of Souper on 269,113 C/C++ programs from SPEC CPU 2017\footnote{https://www.spec.org/cpu2017/}, with and without the addition of their dataflow pruning techniques.
            The compiled outputs from LLVM served as the specifications for Souper.
            They considered a synthesis attempt a success only when it found a candidate with lower cost than the specification;
            they declared it a failure if it took too much time (300 seconds) or memory (4GB).
            They found that their pruning made Souper's synthesis 2.32x faster, and increased the number of successful syntheses finished before timeout by 8\%.
            Their pruning procedure took less than 3\% of the overall runtime of the synthesis attempts in which it was used.
            They found that pruning made synthesis faster for 87\% of problems; it was slower mainly for very large problems with hundreds of instructions.
            }

        %Present CEGIS first, then decide if this is still in
        % \subsubsection{From Relational Verification to SIMD Loop Synthesis}
        % \cite{barthe2013simdloopsynth}

\chapter{Stochastic Search}
    Heretofore we have limited ourselves to superoptimizers which operate in a deterministic regime.
    However, there is another way to think of the problem: as an unguided search in a high-dimensional, irregular search space.
    While this way of thinking loses some desirable properties from more deterministic approaches, such as guaranteed progress or canonical ``solutions" for small problems, it also unlocks a variety of techniques for use in superoptimization, which may scale better\footnotemark\, than deterministic techniques.
    
    \footnotetext{In this context, ``scaling better" means that it becomes expensive more slowly, relative to input size. Remember that the space of possible programs grows exponentially with program length, so any optimizer which attempts to explore all of it will take a very long time for even relatively short programs!}
        
    \subsubsection{Stochastic Superoptimization}
        Markov chain monte carlo (MCMC) sampling is a technique used to sample from complex possibility spaces, first used for superoptimization by Schkufza et al. in \cite{schkufza2013stoke}.
        Essentially, the strategy works by drawing candidate rewrites from a distribution with \comment{favorable properties}{add some math!} (in this case, a bias toward low cost). % explain equation 4 -- cheap candidates have higher probability
        \[
            p(\mathcal{R;T}) = \frac{1}{Z} \mathrm{exp}\left(-\beta \cdot c(\mathcal{R;T})\right)
        \]
        The candidates are then scored by a cost function taking into account estimates of both correctness and performance.
        A candidate which scores better than the current best is always accepted; a worse one is accepted with probability proportional to $1/e^{(c'/c)}$, where $c$ is the cost of the old candidate and $c'$ is the cost of the new one. 
        This is the Metropolis-Hastings algorithm \cite{metropolis1953montecarlo} \cite{hastings1970mcmc}.
            
        Correctness is estimated by the hamming distance (bitwise difference: the number of corresponding bits that differ between the two strings, plus insertions, if needed) between the outputs of the candidate and those of the specification on the set of test cases.
        The test cases are generated based on annotations provided by the user; by default, they are uniformly random bitstrings.
        Cost is estimated by the sum of the (average) latencies of the instructions in the candidate.
            
        There are two phases to the search performed by \textsc{Stoke}:
            what they call synthesis, in which initially random programs are permuted to search for programs equivalent to the specification in disconnected parts of the search space,
            and optimization, in which correct programs are permuted in search of better performance.
        The top 20\% of candidates are then formally verified and tested for performance, and the best candidate is returned.
            
        \green{
        Schkufza et al. test \textsc{Stoke} against 25 problems from Hacker's Delight \cite{warren2013hackers}, and 3 additional problems. 
        Beginning with the output of \texttt{llvm -O0}, it produced output which generally matched and in a few cases significantly exceeded that of \texttt{gcc -O3}.
        For 7 of 28 problems, it discovered programs which were algorithmically distinct from the input specification.
        For 3 of the problems, the synthesis step timed out, meaning that it was unable to derive an equivalent program \textit{de novo}; however, the optimization step was still able to discover a rewrite with good performance.
        }

    \subsubsection{Conditionally Correct Superoptimization}
        One problem with the kind of bitwise verification accomplished by SMT solvers is that in some sense it's actually too exact.
        This has the disadvantages both of added expense for verifying more than necessary, and in rejecting correct but inexactly equivalent candidate optimizations.
        There are multiple reasons that successful optimizations might not be bitwise equivalent to the specification
            -- for example, the differences might only appear in a region of input space which is forbidden for \comment{some reason}{add an example}.
        While there are a number of existing efforts to produce relaxation conditions from annotations given by the programmer to a traditional compiler, another possibility is to attempt to have the compiler generate them itself for review by the programmer.
        Such is the project of \cite{sharma2015conditionally}.
            
        This paper expands on \textsc{Stoke} with the intention of proving correctness more rigorously than the test-case based approach it originally used.
        It does this by augmenting it with a new verification algorithm, which they call \textsc{Cove}.
        This algorithm works using a formal verification system for x86 assembly created by the same authors \cite{sharma2013ddec} (modulo certain pointer aliasing changes) to generate verification conditions for the candidate program, which are then
            \comment{fed into \textsc{Cove}}{the paragraph reads like you're currently describing Cove, so saying the results are fed into Cove is a bit confusing}
            to produce a set of preconditions that make the target and candidate equivalent.
        These preconditions are then presented to the programmer, and if accepted, the candidate program is
            \comment{considered correct}{my immediate question is whether it's actually easier for a programming to look at preconditions than it is for them to look at generated candidate programs}.
            
        The aliasing changes mentioned above constitute a relaxation of the formal verification process -- it turns out that reasoning about
            \comment{aliasing}{I wouldn't assume people know this term}
            is a relatively expensive part of the verifier, and that it is the grounds for rejection for a large number of candidates.
        Therefore, rather than considering every possible case of pointer aliasing, only those which are relevant in the traces for the given test cases are considered.
        This does not affect the overall correctness of \textsc{Cove}, since the derived non-aliasing assumptions are included (explicitly or implicitly) in the final set of preconditions presented to the user.
            
        The preconditions are generated from the verification conditions via
            \comment{abstract interpretation}{describe abstract interpretation briefly}
            by conjoining abstractions of the test inputs on which the target and candidate are equivalent in progressively more-restrictive abstract domains.
        The domains used in \textsc{Cove} are bitvector alignment to 1, 2, 4, 8, 16, 32, and 64 bytes, bit-vector equalities, and bit-vector intervals (interpreted as intervals of unsigned integers).
            
        % \red{TO DO:} ask Greg about what exactly the \comment{abstraction function is doing}{the abstraction function lifts a set of concrete values to the abstract domain--if you think of the abstract domain as some \emph{structured} set of inputs, then the abstract function enforces that structure while preserving overapproximation. The concretization function is the opposite--it ``forgets'' the structure of the abstract domain.}
            
        \red{TODO:} a little background on abstract interpretation
            
        \green{
        Results: 
        }


    \subsubsection{Scaling up Superoptimization}
        Perhaps it's a natural question, when comparing the strengths and weaknesses of various superoptimization techniques, to ask, "why not use them all together, and let the strengths of each cover the weaknesses of the others?"
        Phothilimthana et al. attempt precisely this in \cite{phothilimthana2016scaling}.
        They also spend considerable effort comparing and contrasting results achieved by various techniques.
            
        This paper uses multiple techniques for synthesis in a "cooperative superoptimizer".
        \textsc{Lens}, a new enumerative superoptimization algorithm,
        a sliding-window strategy to decompose programs too large to superoptimize directly,
        and a synthesizer combining communicating symbolic, stochastic, and enumerative search programs.
        Verification is accomplished by an equivalence requirement in the pruning step.
            
        \textsc{Lens} is an enumerative algorithm which uses a bi-directional (pre- and post-conditional) pruning strategy to cut down the search space.
        It works by considering the space of programs to enumerate as composed of a set of equivalence classes of input-output behaviors on a reduced bitwidth, and attempting to bridge the gap between the input and the output of the specification.
        If such a bridge is found, it is verified using a constraint solver;
                if accepted, it's returned,
                but if rejected, the constraint solver returns an input counterexample,
            which is then used as a test case to prune \comment{further candidates}{The description of Lens could be more detailed}.

        The sliding window strategy is fairly simple; it randomly picks a window of the program to superoptimize, attempts to improve it, and repeats until no position of the window yields an improvement to the program.
        There is one additional detail that bears mentioning: 
            this window decomposition allows the verification step to use the context of the surrounding program when checking equivalence;
            rather than checking equivalence of the candidate with the target, it can check the equivalence of the sequence prefix-candidate-suffix with the sequence prefix-target-suffix.
        This allows a few additional optimizations.

        The strategies used in the "cooperative superoptimizer" are
        \begin{itemize}
            \item \textsc{Lens} on the whole program,
            \item \textsc{Lens} with window decomposition,
            \item an SMT solver on the whole program,
            \item an SMT solver with window decomposition,
            \item \textsc{Stoke} starting from a random program, and
            \item \textsc{Stoke} starting from the specification program.
        \end{itemize}
        The communication between synthesizers takes the form of a shared best solution, $p_{best}$.
        The whole-program \textsc{Lens} and SMT synthesizers don't use $p_{best}$. The others restart their synthesis, using $p_{best}$ as the new starting point instead of the target.
        
        \red{TODO:} results
        
        \begin{figure}
            \centering
            \textbf{Your title}\par\medskip
            \includegraphics[scale=0.5]{scaling}
            \caption{Your caption}
        \end{figure}

    % on backburner
    %\subsubsection{Sound Loop Superoptimization for Google Native Client}
    %\cite{churchill2017soundloop}

    \subsection{Reinforcement Learning}
        Reinforcement learning is a machine learning paradigm useful for cases when the structure of a problem is not straightforwardly numeric, but an objective function of the success/failure of an attempted output is available. This function is called the reward/loss function, depending on its sign. In such cases neural networks may be trained by gradient ascent on the reward function or gradient descent on the loss function.
        % REINFORCE: REward Increment = Non-negative Factor $\times$ Offset Reinforcement $\times$ Characteristic Eligibility
        % for RL focus on loss/reward fn

    Due to Sutton et al. in \cite{sutton1999policygradient}, the gradient may be calculated by the following expression:
    \[
        \frac{\partial \rho}{\partial \theta} = \sum_s d^\pi(s) \sum_a \frac{\partial \pi(s,a, \theta)}{\partial \theta}Q^\pi(S,a),
    \]
    where $\theta$ is the vector of policy parameters, $\rho(\pi)$ is the (long-term) expected reward from following policy $\pi$, $d^\pi$ is the (stationary) distribution of states expected under $\pi$, and $Q^\pi(s,a)$ is the long-term expected reward from following policy $\pi$ starting from a given state-action pair.

    This formulation \textit{requires} that $\pi$ be differentiable with respect to $\theta$, i.e. that $\frac{\partial \pi(s,a, \theta)}{\partial \theta}$ exists.
    The reason this formulation is convenient is that there is no term of the form $\frac{\partial d^\pi(s)}{\partial \theta}$, i.e. the effect of policy changes on the distribution of states is irrelevant.
    This allows the gradient to be conveniently estimated by sampling.  % <- ask Greg to please clarify

    \comment{}{(My local copy is not quite up to date here so I don't have text to highlight) The $Q$ function measures the \emph{long-term} rewards starting from a given state-action pair, not the immediate reward.}

    %\subsubsection{Learning Performance-Improving Code Edits} \cite{}

    \subsubsection{Learning to Superoptimize Programs}
        % Augment \textsc{Stoke} by using reinforcement learning to improve the distribution of proposed mutations.
        % two different techniques: learning bias and multilayer perceptron
        One natural objection to the techniques used in \textsc{Stoke} is that in an optimal program, some instructions are more likely than others.
        Bunel et al. attempt to apply this insight in \cite{bunel2017learning}.
        They use reinforcement learning to train two different models of the desired policy:
            an unconditional bias for \textsc{Stoke}'s mutation algorithm,
            and a multi-layer perceptron\footnotemark,
                which attempts to estimate the best mutations to make based on the instructions already present in the candidate under consideration.
        \footnotetext{A perceptron is a simple neural network which classifies its input into predetermined categories.}
        
        \green{
        They train the systems using the expected cost of the output of the system, which can be paraphrased as the following loss function:
        \[
            \mathcal{L}(\theta) = \mathbb{E}_{\{\mathcal{R}\sim q_\theta}[r\{\mathcal{R}\}],
        \]
        where $q_\theta$ is the policy distribution, $r$ is the cost estimator for a rewrite, and $\{\mathcal{R}\}$ is a set of rewrites sampled from the policy distribution.
        }
        
        \green{
        Bunel et al. compare their biased version of \textsc{Stoke} with the original and with an unoptimized reference implementation.
        They find that the biased version significantly outperforms the original.
        On the Hacker's Delight problems, \textsc{Stoke} found programs with on average 53\% of the cost of the unoptimized implementation, while the biased version found programs with 32\% of the cost.
        On a set of random programs, \textsc{Stoke}'s average cost was 78\% of the unoptimized cost, while the biased version's average cost was 63.56%.
        The multi-layer perceptron achieved very slightly higher performance than the bias, improving on it by about 1\%.
        }

    \subsubsection{Deep Symbolic Superoptimization Without Human Knowledge}
        Another reasonable desire when creating a superoptimization system is to manipulate programs directly via neural networks.
        Hui et al. attempt to do this in \cite{hui2020deep}. 
        The goal of this project is to rewrite expression trees to smaller equivalent forms. 
        It uses a three-stage system:
        \begin{itemize}
            \item an encoder network\footnotemark, which produces a vector for each subtree of the input
            \item a subtree selector, which tries to select promising subtrees to focus on
            \item a decoder/simplifier network, which tries to write an equivalent but smaller expression to the input.
        \end{itemize}
            
        \footnotetext{An encoder network maps its input, in this case a tree, to a vector; a decoder naturally does the reverse.}
            
        They train the system using the following reward function:
        \[
            R(\mathcal{T}_I,\mathcal{T}_O) =
                \begin{cases}
                    \gamma^{\text{card}(\mathcal{T}_O)} & \text{ if } \mathcal{T}_I \equiv \mathcal{T}_O,
                \\ -\beta\gamma^{\text{card}(\mathcal{T}_O)} & \text{ otherwise}
                \end{cases}
        \] % explain equivalence relation
        where $\mathcal{T}_I$ is the input specification, $\mathcal{T}_O)$ is the candidate tree, $\gamma$ and $\beta$ are constants with $0 < \gamma < 1$, and $\text{card}(\mathcal{T})$ is the number of nodes in a tree.
        Effectively, this function prioritizes correct output, and between equivalent outputs, prefers shorter ones.
        They use an additional loss function when training the encoder which attempts to force equivalent expressions to have similar encodings, given by
        \[
            L = \frac{1}{|\mathbb{S}|} \sum_{\mathcal{T}_O \in \mathbb{S}} || \boldsymbol{h}(\mathcal{T}_O) - \boldsymbol{h}(\mathcal{T}_I) ||^2_2 \cdot (-1)^{\mathbb{I}[\mathcal{T}_O \not\equiv \mathcal{T}_I]}
        \]
        where $\mathbb{S}$ is a set of outputs generated via the decoder from the same input using beam search\footnotemark, $\boldsymbol{h}(\mathcal{T})$ is the embedding of $\mathcal{T}$ generated by the encoder, and $\mathbb{I}$ is the indicator function, which returns 1 when its argument expression is true, and 0 otherwise.
            
        \footnotetext{Beam search is a greedy search algorithm which attempts to pursue the most-promising candidates first. It discards less-promising candidates, thereby trading guaranteed completion for memory efficiency.}

    \subsubsection{Learning to Superoptimize Real-World Programs}
        Another attempt at a similar problem is made by Shypula et al. in \cite{shypula2022learning}.
        With this paper we return to the assembly superoptimization task which has been our focus through most of this document. 
        They use a transformer model\footnotemark in a supervised learning approach which attempts to learn input-output matching for assembly programs, with unoptimized binaries (\texttt{gcc -O0}) being used as the input examples, and optimized binaries (\texttt{gcc -O3}) being used as the outputs.
        
        \footnotetext{Transformers are a sequence-to-sequence machine learning model introduced in 2017 by Vaswani et al. in \cite{vaswani2017attention}, which translate "words" of the input sequence into vectors so that they can be directly manipulated by a neural net; they are distinct from autoencoders in that they have an additional "attention" mechanism, whereby words from a sliding context window are scored for importance before being processed by the main net.}
            
        The SILO algorithm essentially consists of replacing the outputs from the training set when a better one is found for a given input, then continuing training.
            
        The loss function can be paraphrased as 
        \[ L = P(F_o) \cdot (\text{cost}(F_o) + \lambda \cdot \text{correctness}(F_o) - b(F_I)), \]
        where $P$ is a term representing the probability of generating output function $F_o$, cost($F$) is the sum of the expected latencies for the instructions in $F_o$ (the same cost function as \textsc{Stoke}) plus the sum of the expected latencies of the instructions actually executed by $F_o$ on the test suite, correctness($F$) is 0 if $F$ is equivalent to $F_I$ and 1 otherwise, $\lambda$ is a tuneable penalty for incorrectness (they set it to 50000), and $b(F_I)$ is a baseline cost for the input function.
        
        \green{
        They test the SILO system against REINFORCE alone, and find that SILO improves on \texttt{gcc -O3} on 8.9\% of their test set, as opposed to 0.9\% for REINFORCE.
        They found that REINFORCE consistently made improvements only on the same few problems from the test set, while SILO improved its performance with training.
        }

\chapter{Conclusion}
\red{TODO:} future work?, comparison of techniques, 
% to make it passable, fill out todos
% work on transitions around chapters -- introduction and conclusion for each, make sure the background information sets up the reader to understand what comes after
% to make it pretty decent,

    % do some synthesis of the "overall story" of superoptimization
    % within each chapter put a bit of a conclusion for the chapter
% for even better, add more background and more papers, and ideally implementations

% briefly summarize intro, talk about different approaches again, advantages & disadvantages
% more results-focused
% if I've noticed open directions, point them out

%\appendix
%    \chapter{Appendix 1}

\backmatter
%\nocite{*} 
%\printbibliography
\bibliographystyle{apalike}
\bibliography{bibliography}
% To force capitalization in an article title or where all lowercase is generally used, bracket the capital letter in curly braces.
% optionally, an index would go here.

\end{document}
